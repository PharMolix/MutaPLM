{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/niezk_dair/anaconda3/envs/openbiomed/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** loading protein model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForMutationDesign were not initialized from the model checkpoint at /data3/niezk/model/esm/esm2_t33_650M_UR50D and are newly initialized: ['esm.encoder.layer.32.crossattention_adapter.self.query.bias', 'esm.encoder.layer.32.crossattention_adapter.self.value.bias', 'esm.encoder.layer.32.crossattention_adapter.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.32.crossattention_adapter.self.key.weight', 'esm.encoder.layer.32.crossattention_adapter.self.key.bias', 'mutation_classifier.bias', 'esm.encoder.layer.32.crossattention_adapter.self.value.weight', 'esm.encoder.layer.32.crossattention_adapter.output.dense.weight', 'esm.encoder.layer.32.intermediate_adapter.dense.weight', 'esm.encoder.layer.32.intermediate_adapter.dense.bias', 'esm.encoder.layer.32.crossattention_adapter.LayerNorm.weight', 'esm.encoder.layer.32.output_adapter.dense.bias', 'esm.encoder.layer.32.crossattention_adapter.self.query.weight', 'mutation_classifier.weight', 'esm.encoder.layer.32.LayerNorm_adapter.bias', 'esm.encoder.layer.32.output_adapter.dense.weight', 'esm.encoder.layer.32.LayerNorm_adapter.weight', 'esm.encoder.layer.32.crossattention_adapter.LayerNorm.bias', 'esm.encoder.layer.32.crossattention_adapter.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** freezing protein model...\n",
      "*** loading llm tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** loading llm from /data3/niezk/model/biomedgpt-lm...\n",
      "*** adding LoRA...\n",
      "trainable params: 0 || all params: 6,774,206,464 || trainable%: 0.0\n",
      "*** building delta encoder...\n",
      "*** model built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MutaPLM(\n",
       "  (protein_model): EsmForMutationDesign(\n",
       "    (esm): EsmModel(\n",
       "      (embeddings): EsmEmbeddings(\n",
       "        (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
       "      )\n",
       "      (encoder): EsmEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-31): 32 x EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (32): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (crossattention_adapter): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate_adapter): EsmIntermediate(\n",
       "              (dense): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            )\n",
       "            (output_adapter): EsmOutput(\n",
       "              (dense): Linear(in_features=640, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm_adapter): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (contact_head): EsmContactPredictionHead(\n",
       "        (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (mutation_classifier): Linear(in_features=1280, out_features=2, bias=True)\n",
       "    (lm_head): EsmLMHead(\n",
       "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=1280, out_features=33, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (llm): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(32001, 4096)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm()\n",
       "              (post_attention_layernorm): LlamaRMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler_protein1): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (pooler_protein2): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (proj_protein1): Linear(in_features=1280, out_features=4096, bias=True)\n",
       "  (proj_protein2): Linear(in_features=1280, out_features=4096, bias=True)\n",
       "  (proj_text): Linear(in_features=4096, out_features=1280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from torch.cuda.amp import autocast\n",
    "from model.mutaplm import MutaPLM\n",
    "\n",
    "# load model\n",
    "device = torch.device(\"cuda:2\")\n",
    "model_config_path = \"./configs/mutaplm_inference.yaml\"\n",
    "model_cfg = yaml.load(open(model_config_path, \"r\"), Loader=yaml.Loader)\n",
    "model_cfg[\"device\"] = device\n",
    "model = MutaPLM(**model_cfg).to(device)\n",
    "new_ckpt = torch.load(open(\"./ckpts/mutaplm.pth\", \"rb\"), map_location=\"cpu\")[\"model\"]\n",
    "model.load_state_dict(new_ckpt, strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted function: ase that can recognize specific palindromic sequences and target them to the proteasome for degradation. Can recognize the palindromic IN box and upstream of a 5'-AAA-3' motif in different proteins such as cyclins B1/2 (CCNB1 and CCNB2), histone H4 (H4), and histone H2B (H2B). Exhibits an endogenous activity in HEK293 cells (human embryonic kidney cells) and can induce the degradation of CCNB1 in these cells. Can drive the degradation of CCNB1 in HEK293 cells even if CCNB1 does not contain the IN box in its C-terminus. Also involved in the cell cycle regulation of G1/S phase by controlling KIP1/CHFR-mediated destabilization of CDK4 and phosphorylation of histone H3, histone H4 and histone H2B. Involved in DNA damage response by triggering protein degradation through the 26S proteasome in a p53/TP53-dependent\n",
      "Predicted effect: Decrease of IN box-dependent E3 ubiquitin ligase activity.\n"
     ]
    }
   ],
   "source": [
    "# Explanation: given wildtype protein and mutation site, predict its original function and mutational effect.\n",
    "wildtype_protein = \"MASDAAAEPSSGVTHPPRYVIGYALAPKKQQSFIQPSLVAQAASRGMDLVPVDASQPLAEQGPFHLLIHALYGDDWRAQLVAFAARHPAVPIVDPPHAIDRLHNRISMLQVVSELDHAADQDSTFGIPSQVVVYDAAALADFGLLAALRFPLIAKPLVADGTAKSHKMSLVYHREGLGKLRPPLVLQEFVNHGGVIFKVYVVGGHVTCVKRRSLPDVSPEDDASAQGSVSFSQVSNLPTERTAEEYYGEKSLEDAVVPPAAFINQIAGGLRRALGLQLFNFDMIRDVRAGDRYLVIDINYFPGYAKMPGYETVLTDFFWEMVHKDGVGNQQEEKGANHVVVK\"\n",
    "site = \"A70K\"\n",
    "mutated_protein = wildtype_protein[:int(site[1:-1])-1] + site[-1] + wildtype_protein[int(site[1:-1]):]\n",
    "muta_prompt = f\"Next is a feature of the mutation {site[0]} to {site[-1]} at position {site[1:-1]}. Please generate a brief summary text to describe it.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    with autocast(dtype=torch.bfloat16):\n",
    "        pred_func, pred_mut = model.generate([wildtype_protein], [mutated_protein], [muta_prompt])\n",
    "\n",
    "print(\"Predicted function:\", pred_func[0])\n",
    "print(\"Predicted effect:\", pred_mut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutated position: 70\n",
      "new amino acid: K\n"
     ]
    }
   ],
   "source": [
    "# Engineering: given wildtype protein and mutational effect, predict mutated position and new amino acid.\n",
    "wildtype_protein = \"MASDAAAEPSSGVTHPPRYVIGYALAPKKQQSFIQPSLVAQAASRGMDLVPVDASQPLAEQGPFHLLIHALYGDDWRAQLVAFAARHPAVPIVDPPHAIDRLHNRISMLQVVSELDHAADQDSTFGIPSQVVVYDAAALADFGLLAALRFPLIAKPLVADGTAKSHKMSLVYHREGLGKLRPPLVLQEFVNHGGVIFKVYVVGGHVTCVKRRSLPDVSPEDDASAQGSVSFSQVSNLPTERTAEEYYGEKSLEDAVVPPAAFINQIAGGLRRALGLQLFNFDMIRDVRAGDRYLVIDINYFPGYAKMPGYETVLTDFFWEMVHKDGVGNQQEEKGANHVVVK\"\n",
    "effect_text = \"Strongly enhanced InsP6 kinase activity. The mutation in the ITPK protein causes a change in its catalytic activity.\"\n",
    "muta_prompt = \"What is the mutated position and new amino acid?\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    with autocast(dtype=torch.bfloat16):\n",
    "        preds = model.lm_design([wildtype_protein], [effect_text], muta_prompt=[muta_prompt])\n",
    "\n",
    "top50 = preds[0].flatten().topk(50).indices\n",
    "top50_pos = top50 // len(model.protein_tokenizer)\n",
    "top50_aa = top50 % len(model.protein_tokenizer)\n",
    "top50_aa = model.protein_tokenizer.batch_decode(top50_aa)\n",
    "print(\"mutated position:\", top50_pos[0].item())\n",
    "print(\"new amino acid:\", top50_aa[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbiomed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
